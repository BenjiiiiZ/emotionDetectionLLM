{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook a LLM is used to detect emotions in text. There are multiple parameters you can change for different runs, which can be set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results are saved to a text file, which should be renamed when you have run the entire notebook and want to do so again\n",
    "testFile = \"oneShotTestAggregateLargeB.txt\"\n",
    "# Next parameter is the amount of runs you want the method to do. As LLMs give different results each time, you should run it multiple times to \n",
    "# get more robust results. Setting it to n means n-1 runs\n",
    "methodRuns = 11\n",
    "# The last parameter to set is the threshold for the aggregating of the labels of the annotators\n",
    "# A threshold of n means n different annotators choose this label for this example of text\n",
    "annotatorThreshold = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the LLM is called, some data has to be prepared.\n",
    "First we transform the CSV file with the unannotated data to a dataframe.\n",
    "Next we create a string with the appropriate labels.\n",
    "Lastly we create a dataframe from the annotated examples to test the accuracy with. This dataframe needs to become a numPy array. The annotated data is aggregated by majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "# read the unannotated data file to a dataframe\n",
    "unannotatedDataFrame = pd.read_csv('notAnnotatedData.csv', sep=';', header=0)\n",
    "# create a string with all the labels\n",
    "# approval has an \", as the predictions for disapproval otherwise also get counted for approval\n",
    "labelsToAnnotate: str = \"admiration;amusement;anger;annoyance;\\\"approval;caring;confusion;curiosity;desire;disappointment;disapproval;disgust;embarrassment;excitement;fear;gratitude;grief;joy;love;nervousness;optimism;pride;realization;relief;remorse;sadness;suprise;neutral\"\n",
    "\n",
    "# read the annotated files to dataframes\n",
    "# 5 different annotators annotated the data, this has to be aggregated\n",
    "annotated1 = pd.read_csv('annotatedData/Emotion-1.csv', sep=';', header=0)\n",
    "annotated2 = pd.read_csv('annotatedData/Emotion-2.csv', sep=';', header=0)\n",
    "annotated3 = pd.read_csv('annotatedData/Emotion-3.csv', sep=';', header=0)\n",
    "annotated4 = pd.read_csv('annotatedData/Emotion-4.csv', sep=',', header=0)\n",
    "annotated5 = pd.read_csv('annotatedData/Emotion-5.csv', sep=',', header=0)\n",
    "# drop the not needed columns. Add does not work properly otherwise.\n",
    "dropList = [\"id\", \"question_id\", \"participant_id\", \"frisian\", \"dutch\", \"english\"]\n",
    "annotated1Clean = annotated1.drop(dropList, axis=1)\n",
    "annotated2Clean = annotated2.drop(dropList, axis=1)\n",
    "annotated3Clean = annotated3.drop(dropList, axis=1)\n",
    "annotated4Clean = annotated4.drop(dropList, axis=1)\n",
    "annotated5Clean = annotated5.drop(dropList, axis=1)\n",
    "\n",
    "# combine the data frames\n",
    "annotatedDataFrame12 = annotated1Clean.add(annotated2Clean, fill_value=0)\n",
    "annotatedDataFrame34 = annotated3Clean.add(annotated4Clean, fill_value=0)\n",
    "annotatedDataFrame345 = annotatedDataFrame34.add(annotated5Clean, fill_value=0)\n",
    "annotatedDataFrame = annotatedDataFrame345.add(annotatedDataFrame12, fill_value=0)\n",
    "# replace the NaN with 0, this is needed in order to calculate the F1 score, precision and recall later\n",
    "annotatedDataFrameNoNaN = annotatedDataFrame.fillna(0)\n",
    "\n",
    "# transform the annotated data to a numpy array to do the evaluations with\n",
    "numAnnotatedSummed = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN)\n",
    "# Then a threshold is set on which labels are considered \"true\"\n",
    "numAnnotated = numpy.where(numAnnotatedSummed < annotatorThreshold, 0, 1)\n",
    "\n",
    "# row 0 till 49 are annotated\n",
    "numpy.set_printoptions(threshold=30000)\n",
    "print(numAnnotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the oneshot method on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def oneShot(data) -> str:\n",
    "    \"oneshot method for detecting emotions. A single example is given to the LLM\"\n",
    "    response = ollama.chat(model='llama3', format='json',messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"\"\"  Your task is to identify one to three key emotions in the text using the given labels. \n",
    "                        If no emotion is detected, use the label neutral.\n",
    "                        Answer with the identified emotions in JSON format, without explanation.\n",
    "                   \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"  Text: I would like to see a large field of solar panels, worn by the inhabitants themselves, paid for by the inhabitants themselves, the moment a resident leaves for the area he should be able to sell his part back, new residents should also be able to buy a part. In this way, you as a municipality become self-sufficient while it is affordable for the vast majority of residents. Residents who would not be able to participate financially, must be able to use the solar energy, as they currently purchase electricity from their supplier.\n",
    "                        \\n Labels: \"\"\" + labelsToAnnotate \n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"Emotions: optimism, desire, caring\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Text: \" + data.english + \"\\n Labels: \" + labelsToAnnotate\n",
    "    },\n",
    "    ])\n",
    "    # response is a mapping of: model, created at, message, done, total duration, load duration, prompt eval durationm eval count and eval duration\n",
    "    # we only want to return the content of the message to be used \n",
    "    return pd.Series(response['message']['content'])\n",
    "\n",
    "# we exclude row 4, as that was used for giving examples to the LLM.\n",
    "# this example got clear labels from the annotators and provide a good overall understanding of the different emotions\n",
    "test = unannotatedDataFrame.iloc[0:50]\n",
    "smallTest = test.drop([4])\n",
    "\n",
    "# apply runs the method on each row of the provided dataframe. Data.english is used to only provide the LLM with the english text\n",
    "# progress_apply is used to see the progress of calling the method per row, as the LLM takes some time to run it is nice to see the progress\n",
    "\n",
    "# to create more robust results, the method is run 10 times and the resulting labels are aggregated\n",
    "# if you want to run the method less times, change the following variable: methodruns\n",
    "runs = {}\n",
    "for n in range (1, methodRuns):\n",
    "    runs[\"response{0}\".format(n)] = smallTest.progress_apply(oneShot, axis=1)\n",
    "    runs[\"response\" + str(n)].columns = [\"labels\"]\n",
    "print(runs[\"response1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use apply, we get the response in a DataFrame format. We want to turn this into the same format as our annotated dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the response we create a dataframe in the same format as our annotated labels\n",
    "# First we need a list of labels\n",
    "labelsList = labelsToAnnotate.split(\";\")\n",
    "# Then for each method run\n",
    "for n in range (1, methodRuns):\n",
    "   # Sometimes the LLM returns the labels with uppercases, these should be changed to lowercase\n",
    "   runs[\"response\" + str(n)][\"labels\"] = runs[\"response\" + str(n)][\"labels\"].apply(lambda x: x.lower())\n",
    "   # We go over each of these labels\n",
    "   for label in labelsList:\n",
    "      # And add a column with the correct name by d[][label]\n",
    "      # Then for each row we either assign 1 if the label shows up in the text and 0 if the label does not show up in the text at all.\n",
    "      # As the LLM sometimes gives emotions in the text followed by :null, those are filtered out. Other wrong formats can be filtered as such as well.\n",
    "      runs[\"response\" + str(n)][label] = runs[\"response\" + str(n)].apply(lambda row: 0 if label + \"\\\":null\" in row.labels else 1 if label in row.labels else  0, axis=1) \n",
    "\n",
    "# This prints the results of the first run\n",
    "print(runs[\"response1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created dataframe needs to drop the labels column and is then the same format as our annotated example. \n",
    "After the column is dropped we transform it into a numPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the result data to a numpy array to do evaluation on\n",
    "frameResults = {}\n",
    "numResults = {}\n",
    "\n",
    "# For each method run\n",
    "for n in range (1,methodRuns):\n",
    "    # We drop the unneeded columns, which are the labels given by the LLM\n",
    "    frameResults[\"response\" + str(n)] = runs[\"response\" + str(n)].drop(['labels', \"\"], axis=1)\n",
    "    # then we turn the dataframe into a numpy array\n",
    "    numResults[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults[\"response\" + str(n)])\n",
    "# This prints the cleaned up numpy array results of the first method run\n",
    "print(numResults[\"results1\"])\n",
    "\n",
    "# now we create and write the results to a file\n",
    "# !! rename the file when new results are created !!\n",
    "f = open(testFile, \"a\")\n",
    "for n in range (1, methodRuns):\n",
    "    f.write(\"Results \" + str(n) + \"\\n\")\n",
    "    f.write(pd.DataFrame.to_string(runs[\"response\" + str(n)]) + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults[\"results\" + str(n)]) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to evaluate the results.\n",
    "\n",
    "This is done by calculating the F1 score, recall and precision. \n",
    "As we do not have a fully annotated data set, the score will be calculated not over the entire dataset, but only the annotated part.\n",
    "Also, as the LLM was run multiple times, those results must be aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# The evaluation metrics are calculated over the annotated rows.\n",
    "# The LLM is run on row 0 till 50, excluding row 4 and produces results in rows 0 to 49.\n",
    "numOnlyAnno = numAnnotated[0:50]\n",
    "# as row 4 is used to give examples, it is removed for the evaluation\n",
    "numOnlyAnno = numpy.delete(numOnlyAnno, 4, axis=0)\n",
    "\n",
    "# Now we need to aggregate the results from the different method runs\n",
    "# We start by adding all the numpy arrays together\n",
    "sumResults =  numResults[\"results1\"]\n",
    "for n in range (2, methodRuns):\n",
    "    sumResults = numpy.add(sumResults, numResults[\"results\" + str(n)])\n",
    "print(sumResults)\n",
    "f.write(\"Aggregated results\" + \"\\n\" + numpy.array_str(sumResults))\n",
    "f.write(\"Annotator threshold: \" + str(annotatorThreshold))\n",
    "f.write(\"Number of runs: \" + str(methodRuns))\n",
    "# once they are summed, a threshold is applied\n",
    "# This threshold represents the amount of runs that need to have given a label for a piece of text in order to be considered.\n",
    "for n in range (1, methodRuns):\n",
    "    sumThreshResult = numpy.where(sumResults < n, 0, 1)\n",
    "    print(\"Threshold: \" + str(n))\n",
    "    f.write(\"\\nThreshold: \" + str(n))\n",
    "    # Now that all of the data is properly prepared, the micro F1 score, recall and precision are calculated\n",
    "    f1Score = f1_score(numOnlyAnno, sumThreshResult, average=\"micro\")\n",
    "    recall = recall_score(numOnlyAnno, sumThreshResult, average=\"micro\")\n",
    "    precision = precision_score(numOnlyAnno, sumThreshResult, average=\"micro\")\n",
    "    print(\"micro f1 score: \" + str(f1Score))\n",
    "    print(\"recall: \" + str(recall))\n",
    "    print(\"precision: \" + str(precision))\n",
    "    f.write(\"\\nMicro F1 score: \" + str(f1Score))\n",
    "    f.write(\"\\nMicro recall: \" + str(recall))\n",
    "    f.write(\"\\nMicro precision: \" + str(precision))\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
