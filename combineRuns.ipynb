{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used if you want to combine multiple runs together. Put each run in a CSV file and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oneshotCoT\n",
    "import ollama\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "runs = {}\n",
    "methodRuns = 11\n",
    "zeroshot = 'runs/zeroShotRuns/run'\n",
    "oneshot = 'runs/oneShotRuns/run'\n",
    "fewshot = 'runs/fewShotRuns/run'\n",
    "zeroShotCoT = 'runs/zeroShotCoTRuns/run'\n",
    "oneShotCoT = 'runs/oneShotCoTRuns/run'\n",
    "fewShotCoT = 'runs/fewShotCoTRuns/run'\n",
    "\n",
    "# this part reads each csv to a dataframe and ensures only the labels are copied\n",
    "current = zeroshot\n",
    "for n in range(1, 11):\n",
    "    csvPath = current + str(n) + '.csv'\n",
    "    res = pd.read_csv(csvPath, sep='{', header=None)\n",
    "    res.columns = [\"rows\", \"text\"]\n",
    "    res[[\"labels\", \"num\"]] = res[\"text\"].str.split(\"}\", expand=True)\n",
    "    runs[\"response\" + str(n)] = res[[\"labels\"]].copy()\n",
    "\n",
    "display(runs[\"response2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the response we create a dataframe in the same format as our annotated labels\n",
    "# First we need a list of labels\n",
    "labelsToAnnotate: str = \"admiration;amusement;anger;annoyance;\\\"approval;caring;confusion;curiosity;desire;disappointment;disapproval;disgust;embarrassment;excitement;fear;gratitude;grief;joy;love;nervousness;optimism;pride;realization;relief;remorse;sadness;suprise;neutral\"\n",
    "labelsList = labelsToAnnotate.split(\";\")\n",
    "\n",
    "# Then for each method run\n",
    "for n in range (1, methodRuns):\n",
    "   # Sometimes the LLM returns the labels with uppercases, these should be changed to lowercase\n",
    "   runs[\"response\" + str(n)][\"labels\"] = runs[\"response\" + str(n)][\"labels\"].apply(lambda x: x.lower())\n",
    "   # We go over each of these labels\n",
    "   for label in labelsList:\n",
    "      # And add a column with the correct name by d[][label]\n",
    "      # Then for each row we either assign 1 if the label shows up in the text and 0 if the label does not show up in the text at all.\n",
    "      # As the LLM sometimes gives emotions in the text followed by :0, those are filtered out. Other unneeded info is filtered likewise.\n",
    "      runs[\"response\" + str(n)][label] = runs[\"response\" + str(n)].apply(lambda row: 0 if label + \"\\\": 0,\" in row.labels else 0 if label + \"\\\":0,\" in row.labels else 0 if label + \"\\\": false,\" in row.labels else 1 if label in row.labels else  0, axis=1) \n",
    "\n",
    "# transform the result data to a numpy array to do evaluation on\n",
    "frameResults = {}\n",
    "numResults = {}\n",
    "\n",
    "# For each method run\n",
    "for n in range (1,methodRuns):\n",
    "    # We drop the unneeded columns, which are the labels given by the LLM and a empty column it adds\n",
    "    frameResults[\"response\" + str(n)] = runs[\"response\" + str(n)].drop(['labels'], axis=1)\n",
    "    # then we turn the dataframe into a numpy array\n",
    "    numResults[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults[\"response\" + str(n)])\n",
    "\n",
    "sumResults =  numResults[\"results1\"]\n",
    "for n in range (2, methodRuns):\n",
    "    sumResults = numpy.add(sumResults, numResults[\"results\" + str(n)])\n",
    "\n",
    "# this printed array can be copied over to the resultEvaluation notebook\n",
    "print(repr(sumResults))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the mean and standard deviation per run, use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from statsmodels.stats import inter_rater as irr\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "# The methods need to be compared to the annotated data, for which we need to get the proper array\n",
    "# read the annotated files to dataframes\n",
    "annotated1 = pd.read_csv('annotatedData/Emotion-1.csv', sep=';', header=0)\n",
    "annotated2 = pd.read_csv('annotatedData/Emotion-2.csv', sep=';', header=0)\n",
    "annotated3 = pd.read_csv('annotatedData/Emotion-3.csv', sep=';', header=0)\n",
    "annotated4 = pd.read_csv('annotatedData/Emotion-4.csv', sep=',', header=0)\n",
    "annotated5 = pd.read_csv('annotatedData/Emotion-5.csv', sep=',', header=0)\n",
    "# drop the not needed columns. We care only about the labels, not the text.\n",
    "dropList = [\"id\", \"question_id\", \"participant_id\", \"frisian\", \"dutch\", \"english\"]\n",
    "annotated1Clean = annotated1.drop(dropList, axis=1)\n",
    "annotated2Clean = annotated2.drop(dropList, axis=1)\n",
    "annotated3Clean = annotated3.drop(dropList, axis=1)\n",
    "annotated4Clean = annotated4.drop(dropList, axis=1)\n",
    "annotated5Clean = annotated5.drop(dropList, axis=1)\n",
    "# display(annotated1Clean)\n",
    "# combine the data frames\n",
    "annotatedDataFrame12 = annotated1Clean.add(annotated2Clean, fill_value=0)\n",
    "annotatedDataFrame34 = annotated3Clean.add(annotated4Clean, fill_value=0)\n",
    "annotatedDataFrame345 = annotatedDataFrame34.add(annotated5Clean, fill_value=0)\n",
    "annotatedDataFrame = annotatedDataFrame345.add(annotatedDataFrame12, fill_value=0)\n",
    "# replace the NaN with 0\n",
    "annotatedDataFrameNoNaN = annotatedDataFrame.fillna(0)\n",
    "# transform the annotated data to a numpy array to do the evaluations with\n",
    "numAnnotatedSummed = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN.iloc[0:50])\n",
    "# Then a threshold is set on which labels are considered \"true\"\n",
    "numAnnotated = np.where(numAnnotatedSummed < 1, 0, 1)\n",
    "# print(numAnnotated)\n",
    "labelsToAnnotate: str = \"admiration;amusement;anger;annoyance;approval;caring;confusion;curiosity;desire;disappointment;disapproval;disgust;embarrassment;excitement;fear;gratitude;grief;joy;love;nervousness;optimism;pride;realization;relief;remorse;sadness;suprise;neutral\"\n",
    "labelsList = labelsToAnnotate.split(\";\")\n",
    "# First the methods needed for the evaluation\n",
    "# This method can be applied to a dataframe to get the actual labels instead of just numbers\n",
    "def getLabels(data):\n",
    "    result = \"\"\n",
    "    labelsToAnnotate: str = \"admiration;amusement;anger;annoyance;approval;caring;confusion;curiosity;desire;disappointment;disapproval;disgust;embarrassment;excitement;fear;gratitude;grief;joy;love;nervousness;optimism;pride;realization;relief;remorse;sadness;suprise;neutral\"\n",
    "    labelsList = labelsToAnnotate.split(\";\")\n",
    "    for label in labelsList:\n",
    "        if data[label] >= 1:\n",
    "            result = result + \", \" + label\n",
    "    return result\n",
    "def getNumLabels(resultArray, n):\n",
    "    return np.count_nonzero(resultArray >= n)\n",
    "def calculateAgreement(resultArray):\n",
    "    agg = irr.aggregate_raters(resultArray) # returns a tuple (data, categories)\n",
    "    resultOneShot = irr.fleiss_kappa(agg[0], method='fleiss')\n",
    "    return resultOneShot\n",
    "def calcF1RecallPrecision(annoArray, predictArray):\n",
    "    eval = np.empty(shape=(0,3))\n",
    "    f1Score = f1_score(annoArray, predictArray, average=\"micro\")\n",
    "    recall = recall_score(annoArray, predictArray, average=\"micro\")\n",
    "    precision = precision_score(annoArray, predictArray, average=\"micro\")\n",
    "    return np.append(eval, [f1Score, recall, precision])\n",
    "def calcPrecision(annoArray, predictArray):\n",
    "    return precision_score(annoArray, predictArray, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# for zeroshot, use this\n",
    "numAnno = numAnnotated\n",
    "# for oneshot you should use this, as row 4 is used as an example\n",
    "# numAnno = np.delete(numAnnotated, 4, axis=0)\n",
    "# for fewshot you should use this, as row 4,6,21 are used as an example\n",
    "# numAnno = np.delete(numAnnotated, [4, 6, 21], axis=0)\n",
    "# numresults holds the 10 different runs\n",
    "evalTotal = np.empty(shape=(10,3))\n",
    "precLabels = []\n",
    "numLabels = []\n",
    "for n in range (1, 11):\n",
    "    # for each run calculate the f1, recall and precision\n",
    "    evalTotal[n-1] = calcF1RecallPrecision(numAnno, numResults[\"results\" + str(n)])\n",
    "    # also calculate the number of labels\n",
    "    numLabels.append(getNumLabels(numResults[\"results\" + str(n)], 1))\n",
    "    # also the precision\n",
    "    precLabels.append(calcPrecision(numAnno, numResults[\"results\" + str(n)]))\n",
    "# With the precision and total labels the correct and incorrect labels can be calculated\n",
    "correctLabels = np.multiply(precLabels, numLabels)\n",
    "incorrectLabels = np.subtract(numLabels, correctLabels)\n",
    "precMean = statistics.mean(precLabels)\n",
    "precDev = np.std(precLabels)\n",
    "# this prints the \n",
    "print(precLabels)\n",
    "print(precMean)\n",
    "print(precDev)\n",
    "# Next is calculating the mean and the standard deviation\n",
    "meanLabels = statistics.mean(numLabels)\n",
    "stDev = np.std(numLabels)\n",
    "meanLabelsCor = statistics.mean(correctLabels)\n",
    "stDevCor = np.std(correctLabels)\n",
    "meanLabelsIncor = statistics.mean(incorrectLabels)\n",
    "stDevinCor = np.std(incorrectLabels)\n",
    "# Now print the total amount of labels\n",
    "print(numLabels)\n",
    "print(meanLabels)\n",
    "print(stDev)\n",
    "# And the correct labels\n",
    "print(correctLabels)\n",
    "print(meanLabelsCor)\n",
    "print(stDevCor)\n",
    "# and the incorrect labels\n",
    "print(incorrectLabels)\n",
    "print(meanLabelsIncor)\n",
    "print(stDevinCor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
