{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results are saved to a text file, which should be renamed when you have run the entire notebook and want to do so again\n",
    "testFile = \"fewPerAnnotatorResultsV2re.txt\"\n",
    "# Next parameter is the amount of runs you want the method to do. As LLMs give different results each time, you should run it multiple times to \n",
    "# get more robust results. Setting it to n means n-1 runs\n",
    "methodRuns = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "# read the unannotated data file to a dataframe\n",
    "unannotatedDataFrame = pd.read_csv('notAnnotatedData.csv', sep=';', header=0)\n",
    "# create a string with all the labels\n",
    "# approval has an \", as the predictions for disapproval otherwise also get counted for approval\n",
    "labelsToAnnotate: str = \"admiration;amusement;anger;annoyance;\\\"approval;caring;confusion;curiosity;desire;disappointment;disapproval;disgust;embarrassment;excitement;fear;gratitude;grief;joy;love;nervousness;optimism;pride;realization;relief;remorse;sadness;suprise;neutral\"\n",
    "\n",
    "# read the annotated files to dataframes\n",
    "# 5 different annotators annotated the data, this has to be aggregated\n",
    "annotated1 = pd.read_csv('annotatedData/Emotion-1.csv', sep=';', header=0)\n",
    "annotated2 = pd.read_csv('annotatedData/Emotion-2.csv', sep=';', header=0)\n",
    "annotated3 = pd.read_csv('annotatedData/Emotion-3.csv', sep=';', header=0)\n",
    "annotated4 = pd.read_csv('annotatedData/Emotion-4.csv', sep=',', header=0)\n",
    "annotated5 = pd.read_csv('annotatedData/Emotion-5.csv', sep=',', header=0)\n",
    "# drop the not needed columns. Add does not work properly otherwise.\n",
    "dropList = [\"id\", \"question_id\", \"participant_id\", \"frisian\", \"dutch\", \"english\"]\n",
    "annotated1Clean = annotated1.drop(dropList, axis=1)\n",
    "annotated2Clean = annotated2.drop(dropList, axis=1)\n",
    "annotated3Clean = annotated3.drop(dropList, axis=1)\n",
    "annotated4Clean = annotated4.drop(dropList, axis=1)\n",
    "annotated5Clean = annotated5.drop(dropList, axis=1)\n",
    "\n",
    "# replace the NaN with 0, this is needed in order to calculate the F1 score, precision and recall later\n",
    "annotatedDataFrameNoNaN1 = annotated1Clean.fillna(0)\n",
    "annotatedDataFrameNoNaN2 = annotated2Clean.fillna(0)\n",
    "annotatedDataFrameNoNaN3 = annotated3Clean.fillna(0)\n",
    "annotatedDataFrameNoNaN4 = annotated4Clean.fillna(0)\n",
    "annotatedDataFrameNoNaN5 = annotated5Clean.fillna(0)\n",
    "\n",
    "# display(annotated1.iloc[0:50])\n",
    "# transform the annotated data to a numpy array to do the evaluations with\n",
    "numAnnotated1 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN1)\n",
    "numAnnotated2 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN2)\n",
    "numAnnotated3 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN3)\n",
    "numAnnotated4 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN4)\n",
    "numAnnotated5 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN5)\n",
    "print(numAnnotated1)\n",
    "numOf1 = numpy.count_nonzero(numAnnotated1 >= 1)\n",
    "numOf2 = numpy.count_nonzero(numAnnotated2 >= 1)\n",
    "numOf3 = numpy.count_nonzero(numAnnotated3 >= 1)\n",
    "numOf4 = numpy.count_nonzero(numAnnotated4 >= 1)\n",
    "numOf5 = numpy.count_nonzero(numAnnotated5 >= 1)\n",
    "\n",
    "# print(numOf1)\n",
    "# print(numOf2)\n",
    "# print(numOf3)\n",
    "# print(numOf4)\n",
    "# print(numOf5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def fewPerAnnotator(data) -> str:\n",
    "    \"oneshot method for detecting emotions. A single example is given to the LLM\"\n",
    "    response = ollama.chat(model='llama3', format='json',messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"\"\"  Your task is to identify the top [1-3] key emotions that are most relevant to each annotator's perspective using the provided labels. \n",
    "                    From the perspective of each annotator, identify the key emotions that are most strongly represented in their thoughts and feelings. \n",
    "                    If no emotion is detected, use the label neutral.\n",
    "                    Answer with the identified emotions in JSON format, without explanation.\n",
    "                   \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"  Text: I would like to see a large field of solar panels, worn by the inhabitants themselves, paid for by the inhabitants themselves, the moment a resident leaves for the area he should be able to sell his part back, new residents should also be able to buy a part. In this way, you as a municipality become self-sufficient while it is affordable for the vast majority of residents. Residents who would not be able to participate financially, must be able to use the solar energy, as they currently purchase electricity from their supplier.\n",
    "                        \\n Labels: \"\"\" + labelsToAnnotate \n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"Annotator 1: [optimism, desire, caring], Annotator 2: [approval, desire, optimism], Annotator 3: [caring, optimism], Annotator 4: [caring, excitement, optimism], Annotator 5: [approval, caring, desire, excitement, optimism]\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"  Text: Given the impact of the measures, right behind the front door, it seems better to put the control in the hands of municipalities and residents. The market can go out.\n",
    "                        \\n Labels: \"\"\" + labelsToAnnotate \n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \" Annotator 1: [approval, optimism], Annotator 2: [desire], Annotator 3: [approval, caring, realization], Annotator 4: [nervousness, pride], Annotator 5: [approval, fear, nervousness]\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"  Text: Here I have given 10 points. A fairly large wind farm is already coming near me, between Zurich and Witmarsum.  I think that's enough for SÃºdwest-FryslÃ¢n.  The idea of making SÃºdwest-FryslÃ¢n the supplier of the Netherlands seems to me to be a bad idea.  We have wind here, so let's limit ourselves to windmills. Then other parts of the country can invest in large-scale solar parks with less wind. Please share the burden.\n",
    "                        \\n Labels: \"\"\" + labelsToAnnotate \n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \" Annotator 1: [annoyance, caring, disapproval], Annotator 2: [annoyance, disapproval], Annotator 3: [annoyance, disapproval], Annotator 4: [anger, annoyance, disapproval], Annotator 5: [annoyance, disappointment, disapproval]\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Text: \" + data.english + \"\\n Labels: \" + labelsToAnnotate\n",
    "    },\n",
    "    ])\n",
    "    # response is a mapping of: model, created at, message, done, total duration, load duration, prompt eval durationm eval count and eval duration\n",
    "    # we only want to return the content of the message to be used \n",
    "    return pd.Series(response['message']['content'])\n",
    "\n",
    "# we exclude row 4, 6, and 21 as those are used for giving examples to the LLM.\n",
    "# these examples got clear labels from the annotators and provide a good overall understanding of the different emotions\n",
    "test = unannotatedDataFrame.iloc[0:50]\n",
    "smallTest = test.drop([4, 6, 21])\n",
    "\n",
    "\n",
    "# apply runs the method on each row of the provided dataframe. Data.english is used to only provide the LLM with the english text\n",
    "# progress_apply is used to see the progress of calling the method per row, as the LLM takes some time to run it is nice to see the progress\n",
    "\n",
    "# to create more robust results, the method is run 10 times and the resulting labels are aggregated\n",
    "# if you want to run the method less times, change the following variable:\n",
    "# This can be set at the top of the document\n",
    "# methodRuns = 11\n",
    "runs = {}\n",
    "for n in range (1, methodRuns):\n",
    "    runs[\"response{0}\".format(n)] = smallTest.progress_apply(fewPerAnnotator, axis=1)\n",
    "    runs[\"response\" + str(n)].columns = [\"labels\"]\n",
    "print(runs[\"response1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the response we create a dataframe in the same format as our annotated labels\n",
    "# First we need a list of labels\n",
    "labelsList = labelsToAnnotate.split(\";\")\n",
    "annotator1 = {}\n",
    "annotator2 = {}\n",
    "annotator3 = {}\n",
    "annotator4 = {}\n",
    "annotator5 = {}\n",
    "\n",
    "for n in range (1, methodRuns):\n",
    "   # add a column with a list of the different annotators\n",
    "   runs[\"response\" + str(n)][[\"A0\", \"A1\", \"A2\", \"A3\", \"A4\", \"A5\"]] = runs[\"response\" + str(n)][\"labels\"].str.split(\"Annotator\", expand=True)\n",
    "   # now for each annotator their own labels are added to their own dataframe\n",
    "   # double brackets for [[\"A1\"]] are needed to make sure it copies as a dataframe\n",
    "   annotator1[\"run\" + str(n)] = runs[\"response\" + str(n)][[\"A1\"]].copy()\n",
    "   # there are a few instances where the emotions are not given per annotator, that row is given None, which is replaced with \"\"\n",
    "   annotator1[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator1[\"run\" + str(n)].columns = [\"labels\"]\n",
    "   annotator2[\"run\" + str(n)] = runs[\"response\" + str(n)][[\"A2\"]].copy()\n",
    "   annotator2[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator2[\"run\" + str(n)].columns = [\"labels\"]\n",
    "   annotator3[\"run\" + str(n)] = runs[\"response\" + str(n)][[\"A3\"]].copy()\n",
    "   annotator3[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator3[\"run\" + str(n)].columns = [\"labels\"]\n",
    "   annotator4[\"run\" + str(n)]= runs[\"response\" + str(n)][[\"A4\"]].copy()\n",
    "   annotator4[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator4[\"run\" + str(n)].columns = [\"labels\"]\n",
    "   annotator5[\"run\" + str(n)] = runs[\"response\" + str(n)][[\"A5\"]].copy()\n",
    "   annotator5[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator5[\"run\" + str(n)].columns = [\"labels\"]\n",
    "# Then for each method run\n",
    "for n in range (1, methodRuns):\n",
    "   # We go over each of these labels\n",
    "   for label in labelsList:\n",
    "      # And add a column with the correct name by d[][label]\n",
    "      # Then for each row we either assign 1 if the label shows up in the text and 0 if the label does not show up in the text at all.\n",
    "      # As the LLM sometimes gives emotions in the text followed by :null, those are filtered out.\n",
    "      annotator1[\"run\" + str(n)][label] = annotator1[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "      annotator2[\"run\" + str(n)][label] = annotator2[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "      annotator3[\"run\" + str(n)][label] = annotator3[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "      annotator4[\"run\" + str(n)][label] = annotator4[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "      annotator5[\"run\" + str(n)][label] = annotator5[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "numpy.set_printoptions(threshold=30000)\n",
    "\n",
    "# This prints the results of the first run\n",
    "display(annotator5[\"run1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the result data to a numpy array to do evaluation on\n",
    "frameResults1 = {}\n",
    "frameResults2 = {}\n",
    "frameResults3 = {}\n",
    "frameResults4 = {}\n",
    "frameResults5 = {}\n",
    "numResults1 = {}\n",
    "numResults2 = {}\n",
    "numResults3 = {}\n",
    "numResults4 = {}\n",
    "numResults5 = {}\n",
    "\n",
    "# For each method run\n",
    "for n in range (1,methodRuns):\n",
    "    # We drop the unneeded columns, which are the labels given by the LLM\n",
    "    frameResults1[\"response\" + str(n)] = annotator1[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    frameResults2[\"response\" + str(n)] = annotator2[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    frameResults3[\"response\" + str(n)] = annotator3[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    frameResults4[\"response\" + str(n)] = annotator4[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    frameResults5[\"response\" + str(n)] = annotator5[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    # then we turn the dataframe into a numpy array\n",
    "    numResults1[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults1[\"response\" + str(n)])\n",
    "    numResults2[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults2[\"response\" + str(n)])\n",
    "    numResults3[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults3[\"response\" + str(n)])\n",
    "    numResults4[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults4[\"response\" + str(n)])\n",
    "    numResults5[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults5[\"response\" + str(n)])\n",
    "\n",
    "# This prints the cleaned up numpy array results of the first method run of annotator 1\n",
    "print(numResults1[\"results1\"])\n",
    "\n",
    "# now we create and write the results to a file\n",
    "# !! rename the file when new results are created !!\n",
    "# This should be done at the top, but can be done here as well\n",
    "# testFile = \"oneShotTestAggregate3.txt\"\n",
    "f = open(testFile, \"a\")\n",
    "for n in range (1, methodRuns):\n",
    "    f.write(\"Results \" + str(n) + \"\\n\")\n",
    "    f.write(pd.DataFrame.to_string(runs[\"response\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 1\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults1[\"results\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 2\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults2[\"results\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 3\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults3[\"results\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 4\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults4[\"results\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 5\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults5[\"results\" + str(n)]) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# The evaluation metrics are calculated over the annotated rows.\n",
    "# The LLM is run on row 0 till 50, excluding row 4 and produces results in rows 0 to 48.\n",
    "numOnlyAnno1 = numAnnotated1[0:50]\n",
    "numOnlyAnno2 = numAnnotated2[0:50]\n",
    "numOnlyAnno3 = numAnnotated3[0:50]\n",
    "numOnlyAnno4 = numAnnotated4[0:50]\n",
    "numOnlyAnno5 = numAnnotated5[0:50]\n",
    "# as row 4, 6 and 21 are used to give examples, they are removed for the evaluation\n",
    "numOnlyAnno1 = numpy.delete(numOnlyAnno1, [4, 6, 21], axis=0)\n",
    "numOnlyAnno2 = numpy.delete(numOnlyAnno2, [4, 6, 21], axis=0)\n",
    "numOnlyAnno3 = numpy.delete(numOnlyAnno3, [4, 6, 21], axis=0)\n",
    "numOnlyAnno4 = numpy.delete(numOnlyAnno4, [4, 6, 21], axis=0)\n",
    "numOnlyAnno5 = numpy.delete(numOnlyAnno5, [4, 6, 21], axis=0)\n",
    "\n",
    "# Now we need to aggregate the results from the different method runs\n",
    "# We start by adding all the numpy arrays together\n",
    "sumResults1 =  numResults1[\"results1\"]\n",
    "sumResults2 =  numResults2[\"results1\"]\n",
    "sumResults3 =  numResults3[\"results1\"]\n",
    "sumResults4 =  numResults4[\"results1\"]\n",
    "sumResults5 =  numResults5[\"results1\"]\n",
    "for n in range (2, methodRuns):\n",
    "    sumResults1 = numpy.add(sumResults1, numResults1[\"results\" + str(n)])\n",
    "    sumResults2 = numpy.add(sumResults2, numResults2[\"results\" + str(n)])\n",
    "    sumResults3 = numpy.add(sumResults3, numResults3[\"results\" + str(n)])\n",
    "    sumResults4 = numpy.add(sumResults4, numResults4[\"results\" + str(n)])\n",
    "    sumResults5 = numpy.add(sumResults5, numResults5[\"results\" + str(n)])\n",
    "\n",
    "print(sumResults1)\n",
    "f.write(\"Aggregated results 1\" + \"\\n\" + numpy.array_str(sumResults1))\n",
    "f.write(\"Aggregated results 2\" + \"\\n\" + numpy.array_str(sumResults2))\n",
    "f.write(\"Aggregated results 3\" + \"\\n\" + numpy.array_str(sumResults3))\n",
    "f.write(\"Aggregated results 4\" + \"\\n\" + numpy.array_str(sumResults4))\n",
    "f.write(\"Aggregated results 5\" + \"\\n\" + numpy.array_str(sumResults5))\n",
    "\n",
    "f.write(\"Number of runs: \" + str(methodRuns))\n",
    "# once they are summed, a threshold is applied\n",
    "# This threshold represents the amount of runs that need to have given a label for a piece of text in order to be considered.\n",
    "for n in range (1, methodRuns):\n",
    "    sumThreshResult1 = numpy.where(sumResults1 < n, 0, 1)\n",
    "    sumThreshResult2 = numpy.where(sumResults2 < n, 0, 1)\n",
    "    sumThreshResult3 = numpy.where(sumResults3 < n, 0, 1)\n",
    "    sumThreshResult4 = numpy.where(sumResults4 < n, 0, 1)\n",
    "    sumThreshResult5 = numpy.where(sumResults5 < n, 0, 1)\n",
    "    print(\"Threshold: \" + str(n))\n",
    "    f.write(\"\\nThreshold: \" + str(n))\n",
    "    # Now that all of the data is properly prepared, the micro F1 score, recall and precision are calculated\n",
    "    f1ScoreAnn1 = f1_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    recallAnn1 = recall_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    precisionAnn1 = precision_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 1: \" + str(f1ScoreAnn1))\n",
    "    print(\"recall annotator 1: \" + str(recallAnn1))\n",
    "    print(\"precision annotator 1: \" + str(precisionAnn1))\n",
    "    f.write(\"\\nMicro F1 annotator 1: \" + str(f1ScoreAnn1))\n",
    "    f.write(\"\\nMicro recall annotator 1: \" + str(recallAnn1))\n",
    "    f.write(\"\\nMicro precision annotator 1: \" + str(precisionAnn1))\n",
    "\n",
    "    f1ScoreAnn2 = f1_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    recallAnn2 = recall_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    precisionAnn2 = precision_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 2: \" + str(f1ScoreAnn2))\n",
    "    print(\"recall annotator 2: \" + str(recallAnn2))\n",
    "    print(\"precision annotator 2: \" + str(precisionAnn2))\n",
    "    f.write(\"\\nMicro F1 annotator 2: \" + str(f1ScoreAnn2))\n",
    "    f.write(\"\\nMicro recall annotator 2: \" + str(recallAnn2))\n",
    "    f.write(\"\\nMicro precision annotator 2: \" + str(precisionAnn2))\n",
    "\n",
    "    f1ScoreAnn3 = f1_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    recallAnn3 = recall_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    precisionAnn3 = precision_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 3: \" + str(f1ScoreAnn3))\n",
    "    print(\"recall annotator 3: \" + str(recallAnn3))\n",
    "    print(\"precision annotator 3: \" + str(precisionAnn3))\n",
    "    f.write(\"\\nMicro F1 annotator 3: \" + str(f1ScoreAnn3))\n",
    "    f.write(\"\\nMicro recall annotator 3: \" + str(recallAnn3))\n",
    "    f.write(\"\\nMicro precision annotator 3: \" + str(precisionAnn3))\n",
    "\n",
    "    f1ScoreAnn4 = f1_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    recallAnn4 = recall_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    precisionAnn4 = precision_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 4: \" + str(f1ScoreAnn4))\n",
    "    print(\"recall annotator 4: \" + str(recallAnn4))\n",
    "    print(\"precision annotator 4: \" + str(precisionAnn4))\n",
    "    f.write(\"\\nMicro F1 annotator 4: \" + str(f1ScoreAnn4))\n",
    "    f.write(\"\\nMicro recall annotator 4: \" + str(recallAnn4))\n",
    "    f.write(\"\\nMicro precision annotator 4: \" + str(precisionAnn4))\n",
    "\n",
    "    f1ScoreAnn5 = f1_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    recallAnn5 = recall_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    precisionAnn5 = precision_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 5: \" + str(f1ScoreAnn5))\n",
    "    print(\"recall annotator 5: \" + str(recallAnn5))\n",
    "    print(\"precision annotator 5: \" + str(precisionAnn5))\n",
    "    f.write(\"\\nMicro F1 annotator 5: \" + str(f1ScoreAnn5))\n",
    "    f.write(\"\\nMicro recall annotator 5: \" + str(recallAnn5))\n",
    "    f.write(\"\\nMicro precision annotator 5: \" + str(precisionAnn5))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # counting occurences\n",
    "import numpy as np\n",
    "annotatorsSummed = [sumResults1, sumResults2, sumResults3, sumResults4, sumResults5]\n",
    "\n",
    "for a in annotatorsSummed:\n",
    "    numOf1 = np.count_nonzero(a >= 1)\n",
    "    numOf2 = np.count_nonzero(a >= 2)\n",
    "    numOf3 = np.count_nonzero(a >= 3)\n",
    "    numOf4 = np.count_nonzero(a >= 4)\n",
    "    numOf5 = np.count_nonzero(a >= 5)\n",
    "    numOf6 = np.count_nonzero(a >= 6)\n",
    "    numOf7 = np.count_nonzero(a >= 7)\n",
    "    numOf8 = np.count_nonzero(a >= 8)\n",
    "    numOf9 = np.count_nonzero(a >= 9)\n",
    "    numOf10 = np.count_nonzero(a >= 10)\n",
    "    print(\"Labels for threshold 1: \" + str(numOf1))\n",
    "    print(\"Labels for threshold 2: \" + str(numOf2))\n",
    "    print(\"Labels for threshold 3: \" + str(numOf3))\n",
    "    print(\"Labels for threshold 4: \" + str(numOf4))\n",
    "    print(\"Labels for threshold 5: \" + str(numOf5))\n",
    "    print(\"Labels for threshold 6: \" + str(numOf6))\n",
    "    print(\"Labels for threshold 7: \" + str(numOf7))\n",
    "    print(\"Labels for threshold 8: \" + str(numOf8))\n",
    "    print(\"Labels for threshold 9: \" + str(numOf9))\n",
    "    print(\"Labels for threshold 10: \" + str(numOf10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "zeroShotArray = numpy.array([[1, 1, 4, 0, 4, 1, 2, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [0, 0, 0, 0, 4, 4, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    ", [1, 0, 2, 0, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5]\n",
    ", [3, 0, 0, 0, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [7, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [0, 0, 3, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    ", [0, 0, 10, 0, 8, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [0, 0, 9, 1, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [9, 0, 0, 0, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [10, 0, 0, 0, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9]\n",
    ", [0, 0, 9, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [1, 0, 1, 0, 8, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [0, 0, 0, 0, 7, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [1, 0, 2, 0, 5, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5]\n",
    ", [0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9]\n",
    ", [0, 0, 0, 0, 6, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [0, 0, 9, 1, 8, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [3, 0, 3, 0, 4, 0, 4, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7]\n",
    ", [3, 0, 6, 0, 8, 0, 0, 0, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [1, 0, 1, 0, 8, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [0, 0, 6, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [0, 0, 10, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [3, 0, 1, 0, 5, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7]\n",
    ", [0, 0, 1, 0, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [0, 0, 8, 3, 10, 0, 0, 0, 0, 0, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [0, 0, 8, 0, 5, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [7, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [7, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [1, 1, 4, 1, 8, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2]\n",
    ", [1, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5]\n",
    ", [5, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [0, 0, 8, 0, 9, 1, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [9, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [1, 0, 1, 0, 8, 7, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [0, 0, 7, 3, 4, 0, 1, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [1, 0, 2, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [8, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [5, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 4]\n",
    ", [0, 0, 9, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [5, 0, 0, 10, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [1, 0, 2, 7, 1, 0, 2, 3, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [4, 0, 0, 0, 9, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    ", [3, 0, 0, 0, 9, 2, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [0, 0, 7, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    ", [1, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9]\n",
    ", [9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "sumResults1 =  zeroShotArray\n",
    "sumResults2 =  zeroShotArray\n",
    "sumResults3 =  zeroShotArray\n",
    "sumResults4 =  zeroShotArray\n",
    "sumResults5 =  zeroShotArray\n",
    "\n",
    "numOnlyAnno1 = numAnnotated1[0:50]\n",
    "numOnlyAnno2 = numAnnotated2[0:50]\n",
    "numOnlyAnno3 = numAnnotated3[0:50]\n",
    "numOnlyAnno4 = numAnnotated4[0:50]\n",
    "numOnlyAnno5 = numAnnotated5[0:50]\n",
    "\n",
    "# controll\n",
    "for n in range (1, methodRuns):\n",
    "    sumThreshResult1 = numpy.where(sumResults1 < n, 0, 1)\n",
    "    sumThreshResult2 = numpy.where(sumResults2 < n, 0, 1)\n",
    "    sumThreshResult3 = numpy.where(sumResults3 < n, 0, 1)\n",
    "    sumThreshResult4 = numpy.where(sumResults4 < n, 0, 1)\n",
    "    sumThreshResult5 = numpy.where(sumResults5 < n, 0, 1)\n",
    "    print(\"Threshold: \" + str(n))\n",
    "    # Now that all of the data is properly prepared, the micro F1 score, recall and precision are calculated\n",
    "    f1ScoreAnn1 = f1_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    recallAnn1 = recall_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    precisionAnn1 = precision_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 1: \" + str(f1ScoreAnn1))\n",
    "    print(\"recall annotator 1: \" + str(recallAnn1))\n",
    "    print(\"precision annotator 1: \" + str(precisionAnn1))\n",
    "\n",
    "\n",
    "    f1ScoreAnn2 = f1_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    recallAnn2 = recall_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    precisionAnn2 = precision_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 2: \" + str(f1ScoreAnn2))\n",
    "    print(\"recall annotator 2: \" + str(recallAnn2))\n",
    "    print(\"precision annotator 2: \" + str(precisionAnn2))\n",
    "\n",
    "\n",
    "    f1ScoreAnn3 = f1_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    recallAnn3 = recall_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    precisionAnn3 = precision_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 3: \" + str(f1ScoreAnn3))\n",
    "    print(\"recall annotator 3: \" + str(recallAnn3))\n",
    "    print(\"precision annotator 3: \" + str(precisionAnn3))\n",
    "\n",
    "\n",
    "    f1ScoreAnn4 = f1_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    recallAnn4 = recall_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    precisionAnn4 = precision_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 4: \" + str(f1ScoreAnn4))\n",
    "    print(\"recall annotator 4: \" + str(recallAnn4))\n",
    "    print(\"precision annotator 4: \" + str(precisionAnn4))\n",
    "\n",
    "    f1ScoreAnn5 = f1_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    recallAnn5 = recall_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    precisionAnn5 = precision_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 5: \" + str(f1ScoreAnn5))\n",
    "    print(\"recall annotator 5: \" + str(recallAnn5))\n",
    "    print(\"precision annotator 5: \" + str(precisionAnn5))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part finds and prints the labels annotated by the annotators\n",
    "rowLabels = annotatedDataFrameNoNaN3.iloc[0:50]\n",
    "labelsToAnnotate: str = \"admiration;amusement;anger;annoyance;approval;caring;confusion;curiosity;desire;disappointment;disapproval;disgust;embarrassment;excitement;fear;gratitude;grief;joy;love;nervousness;optimism;pride;realization;relief;remorse;sadness;suprise;neutral\"\n",
    "labelsList = labelsToAnnotate.split(\";\")\n",
    "\n",
    "def getLabels(data):\n",
    "    result = \"\"\n",
    "    for label in labelsList:\n",
    "        if data[label] >= 1:\n",
    "            result = result + \", \" + label\n",
    "    return result\n",
    "\n",
    "labelsNeeded = rowLabels.apply(getLabels, axis=1)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(labelsNeeded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
