{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results are saved to a text file, which should be renamed when you have run the entire notebook and want to do so again\n",
    "testFile = \"fewPerAnnotatorResultsV2re.txt\"\n",
    "# Next parameter is the amount of runs you want the method to do. As LLMs give different results each time, you should run it multiple times to \n",
    "# get more robust results. Setting it to n means n-1 runs\n",
    "methodRuns = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'annotatedData/Emotion-2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# read the annotated files to dataframes\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 5 different annotators annotated the data, this has to be aggregated\u001b[39;00m\n\u001b[0;32m     14\u001b[0m annotated1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotatedData/Emotion-1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m annotated2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mannotatedData/Emotion-2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m annotated3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotatedData/Emotion-3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m annotated4 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotatedData/Emotion-4.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\bente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\bente\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'annotatedData/Emotion-2.csv'"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "# read the unannotated data file to a dataframe\n",
    "unannotatedDataFrame = pd.read_csv('notAnnotatedData.csv', sep=';', header=0)\n",
    "# create a string with all the labels\n",
    "# approval has an \", as the predictions for disapproval otherwise also get counted for approval\n",
    "labelsToAnnotate: str = \"admiration;amusement;anger;annoyance;\\\"approval;caring;confusion;curiosity;desire;disappointment;disapproval;disgust;embarrassment;excitement;fear;gratitude;grief;joy;love;nervousness;optimism;pride;realization;relief;remorse;sadness;suprise;neutral\"\n",
    "\n",
    "# read the annotated files to dataframes\n",
    "# 5 different annotators annotated the data, this has to be aggregated\n",
    "annotated1 = pd.read_csv('annotatedData/Emotion-1.csv', sep=';', header=0)\n",
    "annotated2 = pd.read_csv('annotatedData/Emotion-2.csv', sep=';', header=0)\n",
    "annotated3 = pd.read_csv('annotatedData/Emotion-3.csv', sep=';', header=0)\n",
    "annotated4 = pd.read_csv('annotatedData/Emotion-4.csv', sep=',', header=0)\n",
    "annotated5 = pd.read_csv('annotatedData/Emotion-5.csv', sep=',', header=0)\n",
    "# drop the not needed columns. Add does not work properly otherwise.\n",
    "dropList = [\"id\", \"question_id\", \"participant_id\", \"frisian\", \"dutch\", \"english\"]\n",
    "annotated1Clean = annotated1.drop(dropList, axis=1)\n",
    "annotated2Clean = annotated2.drop(dropList, axis=1)\n",
    "annotated3Clean = annotated3.drop(dropList, axis=1)\n",
    "annotated4Clean = annotated4.drop(dropList, axis=1)\n",
    "annotated5Clean = annotated5.drop(dropList, axis=1)\n",
    "\n",
    "# replace the NaN with 0, this is needed in order to calculate the F1 score, precision and recall later\n",
    "annotatedDataFrameNoNaN1 = annotated1Clean.fillna(0)\n",
    "annotatedDataFrameNoNaN2 = annotated2Clean.fillna(0)\n",
    "annotatedDataFrameNoNaN3 = annotated3Clean.fillna(0)\n",
    "annotatedDataFrameNoNaN4 = annotated4Clean.fillna(0)\n",
    "annotatedDataFrameNoNaN5 = annotated5Clean.fillna(0)\n",
    "\n",
    "# display(annotated1.iloc[0:50])\n",
    "# transform the annotated data to a numpy array to do the evaluations with\n",
    "numAnnotated1 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN1)\n",
    "numAnnotated2 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN2)\n",
    "numAnnotated3 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN3)\n",
    "numAnnotated4 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN4)\n",
    "numAnnotated5 = pd.DataFrame.to_numpy(annotatedDataFrameNoNaN5)\n",
    "print(numAnnotated1)\n",
    "numOf1 = numpy.count_nonzero(numAnnotated1 >= 1)\n",
    "numOf2 = numpy.count_nonzero(numAnnotated2 >= 1)\n",
    "numOf3 = numpy.count_nonzero(numAnnotated3 >= 1)\n",
    "numOf4 = numpy.count_nonzero(numAnnotated4 >= 1)\n",
    "numOf5 = numpy.count_nonzero(numAnnotated5 >= 1)\n",
    "\n",
    "# print(numOf1)\n",
    "# print(numOf2)\n",
    "# print(numOf3)\n",
    "# print(numOf4)\n",
    "# print(numOf5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def fewPerAnnotator(data) -> str:\n",
    "    \"oneshot method for detecting emotions. A single example is given to the LLM\"\n",
    "    response = ollama.chat(model='llama3', format='json',messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"\"\"  Your task is to identify the top [1-3] key emotions that are most relevant to each annotator's perspective using the provided labels. \n",
    "                    From the perspective of each annotator, identify the key emotions that are most strongly represented in their thoughts and feelings. \n",
    "                    If no emotion is detected, use the label neutral.\n",
    "                    Answer with the identified emotions in JSON format, without explanation.\n",
    "                   \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"  Text: I would like to see a large field of solar panels, worn by the inhabitants themselves, paid for by the inhabitants themselves, the moment a resident leaves for the area he should be able to sell his part back, new residents should also be able to buy a part. In this way, you as a municipality become self-sufficient while it is affordable for the vast majority of residents. Residents who would not be able to participate financially, must be able to use the solar energy, as they currently purchase electricity from their supplier.\n",
    "                        \\n Labels: \"\"\" + labelsToAnnotate \n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"Annotator 1: [optimism, desire, caring], Annotator 2: [approval, desire, optimism], Annotator 3: [caring, optimism], Annotator 4: [caring, excitement, optimism], Annotator 5: [approval, caring, desire, excitement, optimism]\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"  Text: Given the impact of the measures, right behind the front door, it seems better to put the control in the hands of municipalities and residents. The market can go out.\n",
    "                        \\n Labels: \"\"\" + labelsToAnnotate \n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \" Annotator 1: [approval, optimism], Annotator 2: [desire], Annotator 3: [approval, caring, realization], Annotator 4: [nervousness, pride], Annotator 5: [approval, fear, nervousness]\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"  Text: Here I have given 10 points. A fairly large wind farm is already coming near me, between Zurich and Witmarsum.  I think that's enough for SÃºdwest-FryslÃ¢n.  The idea of making SÃºdwest-FryslÃ¢n the supplier of the Netherlands seems to me to be a bad idea.  We have wind here, so let's limit ourselves to windmills. Then other parts of the country can invest in large-scale solar parks with less wind. Please share the burden.\n",
    "                        \\n Labels: \"\"\" + labelsToAnnotate \n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \" Annotator 1: [annoyance, caring, disapproval], Annotator 2: [annoyance, disapproval], Annotator 3: [annoyance, disapproval], Annotator 4: [anger, annoyance, disapproval], Annotator 5: [annoyance, disappointment, disapproval]\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Text: \" + data.english + \"\\n Labels: \" + labelsToAnnotate\n",
    "    },\n",
    "    ])\n",
    "    # response is a mapping of: model, created at, message, done, total duration, load duration, prompt eval durationm eval count and eval duration\n",
    "    # we only want to return the content of the message to be used \n",
    "    return pd.Series(response['message']['content'])\n",
    "\n",
    "# we exclude row 4, 6, and 21 as those are used for giving examples to the LLM.\n",
    "# these examples got clear labels from the annotators and provide a good overall understanding of the different emotions\n",
    "test = unannotatedDataFrame.iloc[0:50]\n",
    "smallTest = test.drop([4, 6, 21])\n",
    "\n",
    "\n",
    "# apply runs the method on each row of the provided dataframe. Data.english is used to only provide the LLM with the english text\n",
    "# progress_apply is used to see the progress of calling the method per row, as the LLM takes some time to run it is nice to see the progress\n",
    "\n",
    "# to create more robust results, the method is run 10 times and the resulting labels are aggregated\n",
    "# if you want to run the method less times, change the following variable:\n",
    "# This can be set at the top of the document\n",
    "# methodRuns = 11\n",
    "runs = {}\n",
    "for n in range (1, methodRuns):\n",
    "    runs[\"response{0}\".format(n)] = smallTest.progress_apply(fewPerAnnotator, axis=1)\n",
    "    runs[\"response\" + str(n)].columns = [\"labels\"]\n",
    "print(runs[\"response1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used to read in the runs\n",
    "runs = {}\n",
    "methodRuns = 11\n",
    "\n",
    "# this part reads each csv to a dataframe and ensures only the labels are copied\n",
    "current = 'runs/fewPerAnnotatorRuns/run'\n",
    "for n in range(1, 11):\n",
    "    csvPath = current + str(n) + '.csv'\n",
    "    res = pd.read_csv(csvPath, sep='{', header=None)\n",
    "    res.columns = [\"rows\", \"text\"]\n",
    "    res[[\"labels\", \"num\"]] = res[\"text\"].str.split(\"}\", expand=True)\n",
    "    runs[\"response\" + str(n)] = res[[\"labels\"]].copy()\n",
    "\n",
    "display(runs[\"response2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the response we create a dataframe in the same format as our annotated labels\n",
    "# First we need a list of labels\n",
    "labelsList = labelsToAnnotate.split(\";\")\n",
    "annotator1 = {}\n",
    "annotator2 = {}\n",
    "annotator3 = {}\n",
    "annotator4 = {}\n",
    "annotator5 = {}\n",
    "\n",
    "for n in range (1, methodRuns):\n",
    "   # add a column with a list of the different annotators\n",
    "   runs[\"response\" + str(n)][[\"A0\", \"A1\", \"A2\", \"A3\", \"A4\", \"A5\"]] = runs[\"response\" + str(n)][\"labels\"].str.split(\"Annotator\", expand=True)\n",
    "   # now for each annotator their own labels are added to their own dataframe\n",
    "   # double brackets for [[\"A1\"]] are needed to make sure it copies as a dataframe\n",
    "   annotator1[\"run\" + str(n)] = runs[\"response\" + str(n)][[\"A1\"]].copy()\n",
    "   # there are a few instances where the emotions are not given per annotator, that row is given None, which is replaced with \"\"\n",
    "   annotator1[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator1[\"run\" + str(n)].columns = [\"labels\"]\n",
    "   annotator2[\"run\" + str(n)] = runs[\"response\" + str(n)][[\"A2\"]].copy()\n",
    "   annotator2[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator2[\"run\" + str(n)].columns = [\"labels\"]\n",
    "   annotator3[\"run\" + str(n)] = runs[\"response\" + str(n)][[\"A3\"]].copy()\n",
    "   annotator3[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator3[\"run\" + str(n)].columns = [\"labels\"]\n",
    "   annotator4[\"run\" + str(n)]= runs[\"response\" + str(n)][[\"A4\"]].copy()\n",
    "   annotator4[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator4[\"run\" + str(n)].columns = [\"labels\"]\n",
    "   annotator5[\"run\" + str(n)] = runs[\"response\" + str(n)][[\"A5\"]].copy()\n",
    "   annotator5[\"run\" + str(n)].fillna(\"\", inplace=True)\n",
    "   annotator5[\"run\" + str(n)].columns = [\"labels\"]\n",
    "# Then for each method run\n",
    "for n in range (1, methodRuns):\n",
    "   # We go over each of these labels\n",
    "   for label in labelsList:\n",
    "      # And add a column with the correct name by d[][label]\n",
    "      # Then for each row we either assign 1 if the label shows up in the text and 0 if the label does not show up in the text at all.\n",
    "      # As the LLM sometimes gives emotions in the text followed by :null, those are filtered out.\n",
    "      annotator1[\"run\" + str(n)][label] = annotator1[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "      annotator2[\"run\" + str(n)][label] = annotator2[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "      annotator3[\"run\" + str(n)][label] = annotator3[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "      annotator4[\"run\" + str(n)][label] = annotator4[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "      annotator5[\"run\" + str(n)][label] = annotator5[\"run\" + str(n)].apply(lambda row: 0 if label + \"\\\":null,\" in row.labels else 1 if \"\\\"\" + label in row.labels else  0, axis=1) \n",
    "numpy.set_printoptions(threshold=30000)\n",
    "\n",
    "# This prints the results of the first run\n",
    "display(annotator5[\"run1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the result data to a numpy array to do evaluation on\n",
    "frameResults1 = {}\n",
    "frameResults2 = {}\n",
    "frameResults3 = {}\n",
    "frameResults4 = {}\n",
    "frameResults5 = {}\n",
    "numResults1 = {}\n",
    "numResults2 = {}\n",
    "numResults3 = {}\n",
    "numResults4 = {}\n",
    "numResults5 = {}\n",
    "\n",
    "# For each method run\n",
    "for n in range (1,methodRuns):\n",
    "    # We drop the unneeded columns, which are the labels given by the LLM\n",
    "    frameResults1[\"response\" + str(n)] = annotator1[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    frameResults2[\"response\" + str(n)] = annotator2[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    frameResults3[\"response\" + str(n)] = annotator3[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    frameResults4[\"response\" + str(n)] = annotator4[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    frameResults5[\"response\" + str(n)] = annotator5[\"run\" + str(n)].drop(['labels'], axis=1)\n",
    "    # then we turn the dataframe into a numpy array\n",
    "    numResults1[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults1[\"response\" + str(n)])\n",
    "    numResults2[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults2[\"response\" + str(n)])\n",
    "    numResults3[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults3[\"response\" + str(n)])\n",
    "    numResults4[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults4[\"response\" + str(n)])\n",
    "    numResults5[\"results\" + str(n)] = pd.DataFrame.to_numpy(frameResults5[\"response\" + str(n)])\n",
    "\n",
    "# This prints the cleaned up numpy array results of the first method run of annotator 1\n",
    "print(numResults1[\"results1\"])\n",
    "\n",
    "# now we create and write the results to a file\n",
    "# !! rename the file when new results are created !!\n",
    "# This should be done at the top, but can be done here as well\n",
    "# testFile = \"oneShotTestAggregate3.txt\"\n",
    "f = open(testFile, \"a\")\n",
    "for n in range (1, methodRuns):\n",
    "    f.write(\"Results \" + str(n) + \"\\n\")\n",
    "    f.write(pd.DataFrame.to_string(runs[\"response\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 1\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults1[\"results\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 2\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults2[\"results\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 3\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults3[\"results\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 4\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults4[\"results\" + str(n)]) + \"\\n\")\n",
    "    f.write(\"Annotator 5\" + \"\\n\")\n",
    "    f.write(numpy.array_str(numResults5[\"results\" + str(n)]) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code used to evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats import inter_rater as irr\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "# First the methods needed for the evaluation\n",
    "# This method can be applied to a dataframe to get the actual labels instead of just numbers\n",
    "def getLabels(data):\n",
    "    result = \"\"\n",
    "    labelsToAnnotate: str = \"admiration;amusement;anger;annoyance;approval;caring;confusion;curiosity;desire;disappointment;disapproval;disgust;embarrassment;excitement;fear;gratitude;grief;joy;love;nervousness;optimism;pride;realization;relief;remorse;sadness;suprise;neutral\"\n",
    "    labelsList = labelsToAnnotate.split(\";\")\n",
    "    for label in labelsList:\n",
    "        if data[label] >= 1:\n",
    "            result = result + \", \" + label\n",
    "    return result\n",
    "\n",
    "def getNumLabels(resultArray, n):\n",
    "    return np.count_nonzero(resultArray >= n)\n",
    "\n",
    "def calculateAgreement(resultArray):\n",
    "    agg = irr.aggregate_raters(resultArray) # returns a tuple (data, categories)\n",
    "    resultOneShot = irr.fleiss_kappa(agg[0], method='fleiss')\n",
    "    return resultOneShot\n",
    "\n",
    "def calcF1RecallPrecision(annoArray, predictArray):\n",
    "    eval = np.empty(shape=(0,3))\n",
    "    f1Score = f1_score(annoArray, predictArray, average=\"micro\")\n",
    "    recall = recall_score(annoArray, predictArray, average=\"micro\")\n",
    "    precision = precision_score(annoArray, predictArray, average=\"micro\")\n",
    "    return np.append(eval, [f1Score, recall, precision])\n",
    "\n",
    "def calcF1(annoArray, predictArray):\n",
    "    return f1_score(annoArray, predictArray, average=\"micro\")\n",
    "\n",
    "def calcRecall(annoArray, predictArray):\n",
    "    return recall_score(annoArray, predictArray, average=\"micro\")\n",
    "\n",
    "def calcPrecision(annoArray, predictArray):\n",
    "    return precision_score(annoArray, predictArray, average=\"micro\")\n",
    "\n",
    "# The evaluation metrics are calculated over the annotated rows.\n",
    "# The LLM is run on row 0 till 50, excluding row 4 and produces results in rows 0 to 48.\n",
    "numOnlyAnno1 = numAnnotated1[0:50]\n",
    "numOnlyAnno2 = numAnnotated2[0:50]\n",
    "numOnlyAnno3 = numAnnotated3[0:50]\n",
    "numOnlyAnno4 = numAnnotated4[0:50]\n",
    "numOnlyAnno5 = numAnnotated5[0:50]\n",
    "# as row 4, 6 and 21 are used to give examples, they are removed for the evaluation\n",
    "numOnlyAnno1 = numpy.delete(numOnlyAnno1, [4, 6, 21], axis=0)\n",
    "numOnlyAnno2 = numpy.delete(numOnlyAnno2, [4, 6, 21], axis=0)\n",
    "numOnlyAnno3 = numpy.delete(numOnlyAnno3, [4, 6, 21], axis=0)\n",
    "numOnlyAnno4 = numpy.delete(numOnlyAnno4, [4, 6, 21], axis=0)\n",
    "numOnlyAnno5 = numpy.delete(numOnlyAnno5, [4, 6, 21], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used the evaluate the individual runs\n",
    "\n",
    "import statistics\n",
    "# current annotator and results\n",
    "currentAnno = numOnlyAnno5\n",
    "currentResults = numResults5\n",
    "# numresults holds the 10 different runs\n",
    "evalTotal = np.empty(shape=(10,3))\n",
    "f1Labels = []\n",
    "recallLabels = []\n",
    "precLabels = []\n",
    "numLabels = []\n",
    "for n in range (1, 11):\n",
    "    # for each run calculate the f1, recall and precision\n",
    "    evalTotal[n-1] = calcF1RecallPrecision(currentAnno, currentResults[\"results\" + str(n)])\n",
    "    # also calculate the number of labels\n",
    "    numLabels.append(getNumLabels(currentResults[\"results\" + str(n)], 1))\n",
    "    # also the f1, recall and precision\n",
    "    f1Labels.append(calcF1(currentAnno, currentResults[\"results\" + str(n)]))\n",
    "    recallLabels.append(calcRecall(currentAnno, currentResults[\"results\" + str(n)]))\n",
    "    precLabels.append(calcPrecision(currentAnno, currentResults[\"results\" + str(n)]))\n",
    "# With the precision and total labels the correct and incorrect labels can be calculated\n",
    "correctLabels = np.multiply(precLabels, numLabels)\n",
    "incorrectLabels = np.subtract(numLabels, correctLabels)\n",
    "f1Mean = statistics.mean(f1Labels)\n",
    "f1Dev = np.std(f1Labels)\n",
    "recMean = statistics.mean(recallLabels)\n",
    "recDev = np.std(recallLabels)\n",
    "precMean = statistics.mean(precLabels)\n",
    "precDev = np.std(precLabels)\n",
    "# print(precLabels)\n",
    "print(\"f1\")\n",
    "print(f1Mean)\n",
    "print(f1Dev)\n",
    "print(\"recall\")\n",
    "print(recMean)\n",
    "print(recDev)\n",
    "print(\"precision\")\n",
    "print(precMean)\n",
    "print(precDev)\n",
    "# Next is calculating the mean and the standard deviation\n",
    "meanLabels = statistics.mean(numLabels)\n",
    "stDev = np.std(numLabels)\n",
    "meanLabelsCor = statistics.mean(correctLabels)\n",
    "stDevCor = np.std(correctLabels)\n",
    "meanLabelsIncor = statistics.mean(incorrectLabels)\n",
    "stDevinCor = np.std(incorrectLabels)\n",
    "# print(numLabels)\n",
    "print(\"total labels\")\n",
    "print(meanLabels)\n",
    "print(stDev)\n",
    "# # print(correctLabels)\n",
    "# print(meanLabelsCor)\n",
    "# print(stDevCor)\n",
    "# # print(incorrectLabels)\n",
    "# print(meanLabelsIncor)\n",
    "# print(stDevinCor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below is used to evaluate the aggregated results\n",
    "\n",
    "# Now we need to aggregate the results from the different method runs\n",
    "# We start by adding all the numpy arrays together\n",
    "sumResults1 =  numResults1[\"results1\"]\n",
    "sumResults2 =  numResults2[\"results1\"]\n",
    "sumResults3 =  numResults3[\"results1\"]\n",
    "sumResults4 =  numResults4[\"results1\"]\n",
    "sumResults5 =  numResults5[\"results1\"]\n",
    "for n in range (2, methodRuns):\n",
    "    sumResults1 = numpy.add(sumResults1, numResults1[\"results\" + str(n)])\n",
    "    sumResults2 = numpy.add(sumResults2, numResults2[\"results\" + str(n)])\n",
    "    sumResults3 = numpy.add(sumResults3, numResults3[\"results\" + str(n)])\n",
    "    sumResults4 = numpy.add(sumResults4, numResults4[\"results\" + str(n)])\n",
    "    sumResults5 = numpy.add(sumResults5, numResults5[\"results\" + str(n)])\n",
    "\n",
    "print(sumResults1)\n",
    "f.write(\"Aggregated results 1\" + \"\\n\" + numpy.array_str(sumResults1))\n",
    "f.write(\"Aggregated results 2\" + \"\\n\" + numpy.array_str(sumResults2))\n",
    "f.write(\"Aggregated results 3\" + \"\\n\" + numpy.array_str(sumResults3))\n",
    "f.write(\"Aggregated results 4\" + \"\\n\" + numpy.array_str(sumResults4))\n",
    "f.write(\"Aggregated results 5\" + \"\\n\" + numpy.array_str(sumResults5))\n",
    "\n",
    "f.write(\"Number of runs: \" + str(methodRuns))\n",
    "# once they are summed, a threshold is applied\n",
    "# This threshold represents the amount of runs that need to have given a label for a piece of text in order to be considered.\n",
    "for n in range (1, methodRuns):\n",
    "    sumThreshResult1 = numpy.where(sumResults1 < n, 0, 1)\n",
    "    sumThreshResult2 = numpy.where(sumResults2 < n, 0, 1)\n",
    "    sumThreshResult3 = numpy.where(sumResults3 < n, 0, 1)\n",
    "    sumThreshResult4 = numpy.where(sumResults4 < n, 0, 1)\n",
    "    sumThreshResult5 = numpy.where(sumResults5 < n, 0, 1)\n",
    "    print(\"Threshold: \" + str(n))\n",
    "    f.write(\"\\nThreshold: \" + str(n))\n",
    "    # Now that all of the data is properly prepared, the micro F1 score, recall and precision are calculated\n",
    "    f1ScoreAnn1 = f1_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    recallAnn1 = recall_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    precisionAnn1 = precision_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 1: \" + str(f1ScoreAnn1))\n",
    "    print(\"recall annotator 1: \" + str(recallAnn1))\n",
    "    print(\"precision annotator 1: \" + str(precisionAnn1))\n",
    "    f.write(\"\\nMicro F1 annotator 1: \" + str(f1ScoreAnn1))\n",
    "    f.write(\"\\nMicro recall annotator 1: \" + str(recallAnn1))\n",
    "    f.write(\"\\nMicro precision annotator 1: \" + str(precisionAnn1))\n",
    "\n",
    "    f1ScoreAnn2 = f1_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    recallAnn2 = recall_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    precisionAnn2 = precision_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 2: \" + str(f1ScoreAnn2))\n",
    "    print(\"recall annotator 2: \" + str(recallAnn2))\n",
    "    print(\"precision annotator 2: \" + str(precisionAnn2))\n",
    "    f.write(\"\\nMicro F1 annotator 2: \" + str(f1ScoreAnn2))\n",
    "    f.write(\"\\nMicro recall annotator 2: \" + str(recallAnn2))\n",
    "    f.write(\"\\nMicro precision annotator 2: \" + str(precisionAnn2))\n",
    "\n",
    "    f1ScoreAnn3 = f1_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    recallAnn3 = recall_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    precisionAnn3 = precision_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 3: \" + str(f1ScoreAnn3))\n",
    "    print(\"recall annotator 3: \" + str(recallAnn3))\n",
    "    print(\"precision annotator 3: \" + str(precisionAnn3))\n",
    "    f.write(\"\\nMicro F1 annotator 3: \" + str(f1ScoreAnn3))\n",
    "    f.write(\"\\nMicro recall annotator 3: \" + str(recallAnn3))\n",
    "    f.write(\"\\nMicro precision annotator 3: \" + str(precisionAnn3))\n",
    "\n",
    "    f1ScoreAnn4 = f1_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    recallAnn4 = recall_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    precisionAnn4 = precision_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 4: \" + str(f1ScoreAnn4))\n",
    "    print(\"recall annotator 4: \" + str(recallAnn4))\n",
    "    print(\"precision annotator 4: \" + str(precisionAnn4))\n",
    "    f.write(\"\\nMicro F1 annotator 4: \" + str(f1ScoreAnn4))\n",
    "    f.write(\"\\nMicro recall annotator 4: \" + str(recallAnn4))\n",
    "    f.write(\"\\nMicro precision annotator 4: \" + str(precisionAnn4))\n",
    "\n",
    "    f1ScoreAnn5 = f1_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    recallAnn5 = recall_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    precisionAnn5 = precision_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 5: \" + str(f1ScoreAnn5))\n",
    "    print(\"recall annotator 5: \" + str(recallAnn5))\n",
    "    print(\"precision annotator 5: \" + str(precisionAnn5))\n",
    "    f.write(\"\\nMicro F1 annotator 5: \" + str(f1ScoreAnn5))\n",
    "    f.write(\"\\nMicro recall annotator 5: \" + str(recallAnn5))\n",
    "    f.write(\"\\nMicro precision annotator 5: \" + str(precisionAnn5))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # counting occurences\n",
    "import numpy as np\n",
    "annotatorsSummed = [sumResults1, sumResults2, sumResults3, sumResults4, sumResults5]\n",
    "\n",
    "for a in annotatorsSummed:\n",
    "    numOf1 = np.count_nonzero(a >= 1)\n",
    "    numOf2 = np.count_nonzero(a >= 2)\n",
    "    numOf3 = np.count_nonzero(a >= 3)\n",
    "    numOf4 = np.count_nonzero(a >= 4)\n",
    "    numOf5 = np.count_nonzero(a >= 5)\n",
    "    numOf6 = np.count_nonzero(a >= 6)\n",
    "    numOf7 = np.count_nonzero(a >= 7)\n",
    "    numOf8 = np.count_nonzero(a >= 8)\n",
    "    numOf9 = np.count_nonzero(a >= 9)\n",
    "    numOf10 = np.count_nonzero(a >= 10)\n",
    "    print(\"Labels for threshold 1: \" + str(numOf1))\n",
    "    print(\"Labels for threshold 2: \" + str(numOf2))\n",
    "    print(\"Labels for threshold 3: \" + str(numOf3))\n",
    "    print(\"Labels for threshold 4: \" + str(numOf4))\n",
    "    print(\"Labels for threshold 5: \" + str(numOf5))\n",
    "    print(\"Labels for threshold 6: \" + str(numOf6))\n",
    "    print(\"Labels for threshold 7: \" + str(numOf7))\n",
    "    print(\"Labels for threshold 8: \" + str(numOf8))\n",
    "    print(\"Labels for threshold 9: \" + str(numOf9))\n",
    "    print(\"Labels for threshold 10: \" + str(numOf10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to compare calculate results compared to zeroshot, so when the llm is given no examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "zeroShotArray = numpy.array([[1, 1, 4, 0, 4, 1, 2, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [0, 0, 0, 0, 4, 4, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    ", [1, 0, 2, 0, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5]\n",
    ", [3, 0, 0, 0, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [7, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [0, 0, 3, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    ", [0, 0, 10, 0, 8, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [0, 0, 9, 1, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [9, 0, 0, 0, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [10, 0, 0, 0, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9]\n",
    ", [0, 0, 9, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [1, 0, 1, 0, 8, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [0, 0, 0, 0, 7, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [1, 0, 2, 0, 5, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5]\n",
    ", [0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9]\n",
    ", [0, 0, 0, 0, 6, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [0, 0, 9, 1, 8, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [3, 0, 3, 0, 4, 0, 4, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7]\n",
    ", [3, 0, 6, 0, 8, 0, 0, 0, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [1, 0, 1, 0, 8, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [0, 0, 6, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [0, 0, 10, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [3, 0, 1, 0, 5, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]\n",
    ", [0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7]\n",
    ", [0, 0, 1, 0, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [0, 0, 8, 3, 10, 0, 0, 0, 0, 0, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [0, 0, 8, 0, 5, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [7, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [7, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [1, 1, 4, 1, 8, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2]\n",
    ", [1, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5]\n",
    ", [5, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [0, 0, 8, 0, 9, 1, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [9, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [1, 0, 1, 0, 8, 7, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [0, 0, 7, 3, 4, 0, 1, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    ", [1, 0, 2, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [8, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [5, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 4]\n",
    ", [0, 0, 9, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [5, 0, 0, 10, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ", [1, 0, 2, 7, 1, 0, 2, 3, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
    ", [4, 0, 0, 0, 9, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    ", [3, 0, 0, 0, 9, 2, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3]\n",
    ", [0, 0, 7, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]\n",
    ", [1, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9]\n",
    ", [9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "sumResults1 =  zeroShotArray\n",
    "sumResults2 =  zeroShotArray\n",
    "sumResults3 =  zeroShotArray\n",
    "sumResults4 =  zeroShotArray\n",
    "sumResults5 =  zeroShotArray\n",
    "\n",
    "numOnlyAnno1 = numAnnotated1[0:50]\n",
    "numOnlyAnno2 = numAnnotated2[0:50]\n",
    "numOnlyAnno3 = numAnnotated3[0:50]\n",
    "numOnlyAnno4 = numAnnotated4[0:50]\n",
    "numOnlyAnno5 = numAnnotated5[0:50]\n",
    "\n",
    "# controll\n",
    "for n in range (1, methodRuns):\n",
    "    sumThreshResult1 = numpy.where(sumResults1 < n, 0, 1)\n",
    "    sumThreshResult2 = numpy.where(sumResults2 < n, 0, 1)\n",
    "    sumThreshResult3 = numpy.where(sumResults3 < n, 0, 1)\n",
    "    sumThreshResult4 = numpy.where(sumResults4 < n, 0, 1)\n",
    "    sumThreshResult5 = numpy.where(sumResults5 < n, 0, 1)\n",
    "    print(\"Threshold: \" + str(n))\n",
    "    # Now that all of the data is properly prepared, the micro F1 score, recall and precision are calculated\n",
    "    f1ScoreAnn1 = f1_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    recallAnn1 = recall_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    precisionAnn1 = precision_score(numOnlyAnno1, sumThreshResult1, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 1: \" + str(f1ScoreAnn1))\n",
    "    print(\"recall annotator 1: \" + str(recallAnn1))\n",
    "    print(\"precision annotator 1: \" + str(precisionAnn1))\n",
    "\n",
    "\n",
    "    f1ScoreAnn2 = f1_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    recallAnn2 = recall_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    precisionAnn2 = precision_score(numOnlyAnno2, sumThreshResult2, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 2: \" + str(f1ScoreAnn2))\n",
    "    print(\"recall annotator 2: \" + str(recallAnn2))\n",
    "    print(\"precision annotator 2: \" + str(precisionAnn2))\n",
    "\n",
    "\n",
    "    f1ScoreAnn3 = f1_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    recallAnn3 = recall_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    precisionAnn3 = precision_score(numOnlyAnno3, sumThreshResult3, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 3: \" + str(f1ScoreAnn3))\n",
    "    print(\"recall annotator 3: \" + str(recallAnn3))\n",
    "    print(\"precision annotator 3: \" + str(precisionAnn3))\n",
    "\n",
    "\n",
    "    f1ScoreAnn4 = f1_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    recallAnn4 = recall_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    precisionAnn4 = precision_score(numOnlyAnno4, sumThreshResult4, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 4: \" + str(f1ScoreAnn4))\n",
    "    print(\"recall annotator 4: \" + str(recallAnn4))\n",
    "    print(\"precision annotator 4: \" + str(precisionAnn4))\n",
    "\n",
    "    f1ScoreAnn5 = f1_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    recallAnn5 = recall_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    precisionAnn5 = precision_score(numOnlyAnno5, sumThreshResult5, average=\"micro\")\n",
    "    print(\"micro f1 score annotator 5: \" + str(f1ScoreAnn5))\n",
    "    print(\"recall annotator 5: \" + str(recallAnn5))\n",
    "    print(\"precision annotator 5: \" + str(precisionAnn5))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part finds and prints the labels annotated by the annotators\n",
    "rowLabels = annotatedDataFrameNoNaN3.iloc[0:50]\n",
    "labelsToAnnotate: str = \"admiration;amusement;anger;annoyance;approval;caring;confusion;curiosity;desire;disappointment;disapproval;disgust;embarrassment;excitement;fear;gratitude;grief;joy;love;nervousness;optimism;pride;realization;relief;remorse;sadness;suprise;neutral\"\n",
    "labelsList = labelsToAnnotate.split(\";\")\n",
    "\n",
    "def getLabels(data):\n",
    "    result = \"\"\n",
    "    for label in labelsList:\n",
    "        if data[label] >= 1:\n",
    "            result = result + \", \" + label\n",
    "    return result\n",
    "\n",
    "labelsNeeded = rowLabels.apply(getLabels, axis=1)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(labelsNeeded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
